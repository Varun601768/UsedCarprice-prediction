{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8978cb78-81a0-4c30-807f-2e23b4127157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\varun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\varun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lightgbm) (1.13.0)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 948.7 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 948.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.3/1.5 MB 838.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 855.0 kB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3f5de2b-ad92-4367-8668-ac8afb338118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Advanced Vehicle System...\n",
      "Generating advanced dataset...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1015\u001b[0m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;66;03m# Generate comprehensive dataset\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating advanced dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1015\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mvehicle_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_advanced_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated dataset with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# Train all models\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 152\u001b[0m, in \u001b[0;36mAdvancedVehicleSystem.generate_advanced_dataset\u001b[1;34m(self, n_samples)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Basic features\u001b[39;00m\n\u001b[0;32m    151\u001b[0m years \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1990\u001b[39m, \u001b[38;5;241m2025\u001b[39m, n_samples)\n\u001b[1;32m--> 152\u001b[0m makes \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mToyota\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHonda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFord\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mChevrolet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBMW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMercedes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAudi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNissan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHyundai\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKia\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVolkswagen\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSubaru\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMazda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLexus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAcura\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mInfiniti\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCadillac\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLincoln\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJaguar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLand Rover\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m    156\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.09\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.08\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.07\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.06\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.04\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.04\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m0.04\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.03\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.03\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.03\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m body_types \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSedan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTruck\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoupe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHatchback\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvertible\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m    161\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWagon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinivan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrossover\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    162\u001b[0m ], n_samples, p\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.20\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.10\u001b[39m, \u001b[38;5;241m0.08\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.07\u001b[39m])\n\u001b[0;32m    164\u001b[0m fuel_types \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGasoline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHybrid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElectric\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDiesel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlug-in Hybrid\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    166\u001b[0m ], n_samples, p\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.70\u001b[39m, \u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.08\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.02\u001b[39m])\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:975\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core ML Libraries\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n",
    "                             ExtraTreesRegressor, AdaBoostRegressor, BaggingRegressor)\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, ElasticNet, \n",
    "                                 HuberRegressor, RANSACRegressor)\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, \n",
    "                                   GridSearchCV, RandomizedSearchCV, KFold)\n",
    "from sklearn.preprocessing import (StandardScaler, MinMaxScaler, RobustScaler, \n",
    "                                 LabelEncoder, PolynomialFeatures, PowerTransformer)\n",
    "from sklearn.metrics import (mean_absolute_error, mean_squared_error, r2_score, \n",
    "                           mean_absolute_percentage_error, explained_variance_score)\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "class AdvancedEnsembleRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Advanced ensemble regressor with dynamic weighting and stacking\n",
    "    \"\"\"\n",
    "    def __init__(self, base_models=None, meta_model=None, use_stacking=True):\n",
    "        self.base_models = base_models or []\n",
    "        self.meta_model = meta_model or Ridge(alpha=0.1)\n",
    "        self.use_stacking = use_stacking\n",
    "        self.weights = None\n",
    "        self.trained_models = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Train base models\n",
    "        self.trained_models = []\n",
    "        base_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for i, model in enumerate(self.base_models):\n",
    "            model_copy = model.__class__(**model.get_params())\n",
    "            model_copy.fit(X, y)\n",
    "            self.trained_models.append(model_copy)\n",
    "            base_predictions[:, i] = model_copy.predict(X)\n",
    "        \n",
    "        if self.use_stacking:\n",
    "            # Train meta-model on base predictions\n",
    "            self.meta_model.fit(base_predictions, y)\n",
    "        else:\n",
    "            # Calculate dynamic weights based on individual model performance\n",
    "            self.weights = []\n",
    "            for i, model in enumerate(self.trained_models):\n",
    "                pred = base_predictions[:, i]\n",
    "                mse = mean_squared_error(y, pred)\n",
    "                weight = 1 / (mse + 1e-10)  # Inverse of MSE\n",
    "                self.weights.append(weight)\n",
    "            \n",
    "            # Normalize weights\n",
    "            total_weight = sum(self.weights)\n",
    "            self.weights = [w / total_weight for w in self.weights]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        base_predictions = np.zeros((X.shape[0], len(self.trained_models)))\n",
    "        \n",
    "        for i, model in enumerate(self.trained_models):\n",
    "            base_predictions[:, i] = model.predict(X)\n",
    "        \n",
    "        if self.use_stacking:\n",
    "            return self.meta_model.predict(base_predictions)\n",
    "        else:\n",
    "            # Weighted average\n",
    "            return np.average(base_predictions, axis=1, weights=self.weights)\n",
    "\n",
    "class AdvancedVehicleSystem:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.label_encoders = {}\n",
    "        self.feature_selectors = {}\n",
    "        self.model_performances = {}\n",
    "        self.best_model = None\n",
    "        self.feature_importance = {}\n",
    "        \n",
    "        # Initialize all algorithms\n",
    "        self.algorithms = {\n",
    "            'random_forest': RandomForestRegressor(n_estimators=200, max_depth=15, random_state=42),\n",
    "            'gradient_boosting': GradientBoostingRegressor(n_estimators=200, max_depth=8, random_state=42),\n",
    "            'xgboost': xgb.XGBRegressor(n_estimators=200, max_depth=8, random_state=42),\n",
    "            'lightgbm': lgb.LGBMRegressor(n_estimators=200, max_depth=8, random_state=42),\n",
    "            'extra_trees': ExtraTreesRegressor(n_estimators=200, max_depth=15, random_state=42),\n",
    "            'ada_boost': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "            'svr_rbf': SVR(kernel='rbf', C=100, gamma='scale'),\n",
    "            'svr_linear': SVR(kernel='linear', C=100),\n",
    "            'knn': KNeighborsRegressor(n_neighbors=10, weights='distance'),\n",
    "            'mlp': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42),\n",
    "            'ridge': Ridge(alpha=1.0),\n",
    "            'lasso': Lasso(alpha=0.1),\n",
    "            'elastic_net': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "            'huber': HuberRegressor(epsilon=1.35),\n",
    "            'ransac': RANSACRegressor(random_state=42),\n",
    "            'bagging': BaggingRegressor(n_estimators=100, random_state=42)\n",
    "        }\n",
    "        \n",
    "        # Advanced ensemble configurations\n",
    "        self.ensemble_configs = {\n",
    "            'voting_ensemble': [\n",
    "                self.algorithms['random_forest'],\n",
    "                self.algorithms['gradient_boosting'],\n",
    "                self.algorithms['xgboost']\n",
    "            ],\n",
    "            'stacking_ensemble': [\n",
    "                self.algorithms['random_forest'],\n",
    "                self.algorithms['gradient_boosting'],\n",
    "                self.algorithms['xgboost'],\n",
    "                self.algorithms['lightgbm'],\n",
    "                self.algorithms['extra_trees']\n",
    "            ],\n",
    "            'meta_ensemble': [\n",
    "                self.algorithms['random_forest'],\n",
    "                self.algorithms['gradient_boosting'],\n",
    "                self.algorithms['xgboost'],\n",
    "                self.algorithms['lightgbm'],\n",
    "                self.algorithms['svr_rbf'],\n",
    "                self.algorithms['mlp']\n",
    "            ]\n",
    "        }\n",
    "\n",
    "     \n",
    "    # Fix for the generate_advanced_dataset method\n",
    "# Replace the existing probability arrays with these corrected versions:\n",
    "\n",
    "def generate_advanced_dataset(self, n_samples=50000):\n",
    "    \"\"\"\n",
    "    Generate comprehensive synthetic dataset with realistic features\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Basic features - FIXED PROBABILITIES\n",
    "    years = np.random.randint(1990, 2025, n_samples)\n",
    "    \n",
    "    # Fixed make probabilities (sum = 1.0)\n",
    "    make_probs = [0.12, 0.11, 0.10, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.04,\n",
    "                  0.04, 0.03, 0.03, 0.03, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01]\n",
    "    make_probs = np.array(make_probs)\n",
    "    make_probs = make_probs / make_probs.sum()  # Normalize to ensure sum = 1\n",
    "    \n",
    "    makes = np.random.choice([\n",
    "        'Toyota', 'Honda', 'Ford', 'Chevrolet', 'BMW', 'Mercedes', 'Audi', \n",
    "        'Nissan', 'Hyundai', 'Kia', 'Volkswagen', 'Subaru', 'Mazda', 'Lexus',\n",
    "        'Acura', 'Infiniti', 'Cadillac', 'Lincoln', 'Jaguar', 'Land Rover'\n",
    "    ], n_samples, p=make_probs)\n",
    "    \n",
    "    # Fixed body type probabilities\n",
    "    body_probs = [0.25, 0.20, 0.15, 0.10, 0.08, 0.05, 0.05, 0.05, 0.07]\n",
    "    body_probs = np.array(body_probs)\n",
    "    body_probs = body_probs / body_probs.sum()\n",
    "    \n",
    "    body_types = np.random.choice([\n",
    "        'Sedan', 'SUV', 'Truck', 'Coupe', 'Hatchback', 'Convertible', \n",
    "        'Wagon', 'Minivan', 'Crossover'\n",
    "    ], n_samples, p=body_probs)\n",
    "    \n",
    "    # Fixed fuel type probabilities\n",
    "    fuel_probs = [0.70, 0.15, 0.08, 0.05, 0.02]\n",
    "    fuel_probs = np.array(fuel_probs)\n",
    "    fuel_probs = fuel_probs / fuel_probs.sum()\n",
    "    \n",
    "    fuel_types = np.random.choice([\n",
    "        'Gasoline', 'Hybrid', 'Electric', 'Diesel', 'Plug-in Hybrid'\n",
    "    ], n_samples, p=fuel_probs)\n",
    "    \n",
    "    # Fixed transmission probabilities\n",
    "    trans_probs = [0.75, 0.15, 0.08, 0.02]\n",
    "    trans_probs = np.array(trans_probs)\n",
    "    trans_probs = trans_probs / trans_probs.sum()\n",
    "    \n",
    "    transmissions = np.random.choice([\n",
    "        'Automatic', 'Manual', 'CVT', 'Semi-Automatic'\n",
    "    ], n_samples, p=trans_probs)\n",
    "    \n",
    "    # Fixed drivetrain probabilities\n",
    "    drive_probs = [0.45, 0.25, 0.20, 0.10]\n",
    "    drive_probs = np.array(drive_probs)\n",
    "    drive_probs = drive_probs / drive_probs.sum()\n",
    "    \n",
    "    drivetrains = np.random.choice([\n",
    "        'FWD', 'RWD', 'AWD', '4WD'\n",
    "    ], n_samples, p=drive_probs)\n",
    "    \n",
    "    # Fixed condition probabilities\n",
    "    cond_probs = [0.05, 0.15, 0.35, 0.30, 0.15]\n",
    "    cond_probs = np.array(cond_probs)\n",
    "    cond_probs = cond_probs / cond_probs.sum()\n",
    "    \n",
    "    conditions = np.random.choice([\n",
    "        'Poor', 'Fair', 'Good', 'Very Good', 'Excellent'\n",
    "    ], n_samples, p=cond_probs)\n",
    "    \n",
    "    # Fixed engine size probabilities\n",
    "    engine_probs = [0.05, 0.15, 0.20, 0.20, 0.15, 0.10, 0.08, 0.05, 0.02]\n",
    "    engine_probs = np.array(engine_probs)\n",
    "    engine_probs = engine_probs / engine_probs.sum()\n",
    "    \n",
    "    engine_sizes = np.random.choice([\n",
    "        '1.0L', '1.5L', '2.0L', '2.5L', '3.0L', '3.5L', '4.0L', '5.0L', '6.0L+'\n",
    "    ], n_samples, p=engine_probs)\n",
    "    \n",
    "    # Fixed cylinder probabilities\n",
    "    cyl_probs = [0.05, 0.50, 0.30, 0.12, 0.02, 0.01]\n",
    "    cyl_probs = np.array(cyl_probs)\n",
    "    cyl_probs = cyl_probs / cyl_probs.sum()\n",
    "    \n",
    "    cylinders = np.random.choice([3, 4, 6, 8, 10, 12], n_samples, p=cyl_probs)\n",
    "    \n",
    "    # Fixed state probabilities\n",
    "    state_probs = [0.12, 0.09, 0.06, 0.06, 0.04, 0.04, 0.04, 0.03, 0.03, 0.03, 0.46]\n",
    "    state_probs = np.array(state_probs)\n",
    "    state_probs = state_probs / state_probs.sum()\n",
    "    \n",
    "    states = np.random.choice([\n",
    "        'CA', 'TX', 'FL', 'NY', 'PA', 'IL', 'OH', 'GA', 'NC', 'MI', 'Others'\n",
    "    ], n_samples, p=state_probs)\n",
    "    \n",
    "    # Fixed market demand probabilities\n",
    "    demand_probs = [0.3, 0.5, 0.2]\n",
    "    demand_probs = np.array(demand_probs)\n",
    "    demand_probs = demand_probs / demand_probs.sum()\n",
    "    \n",
    "    market_demand = np.random.choice(['Low', 'Medium', 'High'], n_samples, p=demand_probs)\n",
    "    \n",
    "    # Rest of the method remains the same...\n",
    "    # (Continue with the existing mileage calculation, price calculation, etc.)\n",
    "    def _calculate_realistic_prices(self, years, makes, body_types, fuel_types, transmissions, \n",
    "                                  drivetrains, conditions, engine_sizes, cylinders, mileages, \n",
    "                                  states, accident_counts, service_records, prev_owners, \n",
    "                                  market_demand, seasonal_factor):\n",
    "        \"\"\"\n",
    "        Calculate realistic vehicle prices based on multiple factors\n",
    "        \"\"\"\n",
    "        n_samples = len(years)\n",
    "        \n",
    "        # Base MSRP by make and body type\n",
    "        make_multipliers = {\n",
    "            'Toyota': 1.0, 'Honda': 0.95, 'Ford': 0.85, 'Chevrolet': 0.80,\n",
    "            'BMW': 1.8, 'Mercedes': 1.9, 'Audi': 1.7, 'Nissan': 0.90,\n",
    "            'Hyundai': 0.75, 'Kia': 0.70, 'Volkswagen': 1.1, 'Subaru': 1.05,\n",
    "            'Mazda': 0.95, 'Lexus': 1.6, 'Acura': 1.4, 'Infiniti': 1.3,\n",
    "            'Cadillac': 1.5, 'Lincoln': 1.4, 'Jaguar': 2.0, 'Land Rover': 2.1\n",
    "        }\n",
    "        \n",
    "        body_multipliers = {\n",
    "            'Sedan': 1.0, 'SUV': 1.3, 'Truck': 1.2, 'Coupe': 1.1,\n",
    "            'Hatchback': 0.9, 'Convertible': 1.4, 'Wagon': 1.05,\n",
    "            'Minivan': 1.15, 'Crossover': 1.25\n",
    "        }\n",
    "        \n",
    "        # Calculate base prices\n",
    "        base_msrp = 25000  # Base MSRP\n",
    "        make_factors = np.array([make_multipliers.get(make, 1.0) for make in makes])\n",
    "        body_factors = np.array([body_multipliers.get(body, 1.0) for body in body_types])\n",
    "        \n",
    "        # Year factor (newer cars worth more)\n",
    "        year_factors = 1 + (years - 2000) * 0.02\n",
    "        \n",
    "        # Calculate initial price\n",
    "        initial_prices = base_msrp * make_factors * body_factors * year_factors\n",
    "        \n",
    "        # Apply depreciation\n",
    "        vehicle_ages = 2025 - years\n",
    "        depreciation = np.power(0.85, vehicle_ages)  # 15% depreciation per year\n",
    "        \n",
    "        # Mileage factor\n",
    "        mileage_factors = np.exp(-mileages / 200000)  # Exponential decay with mileage\n",
    "        \n",
    "        # Condition multipliers\n",
    "        condition_multipliers = {\n",
    "            'Poor': 0.6, 'Fair': 0.8, 'Good': 1.0, 'Very Good': 1.2, 'Excellent': 1.4\n",
    "        }\n",
    "        condition_factors = np.array([condition_multipliers[cond] for cond in conditions])\n",
    "        \n",
    "        # Fuel type adjustments\n",
    "        fuel_multipliers = {\n",
    "            'Gasoline': 1.0, 'Hybrid': 1.15, 'Electric': 1.25, \n",
    "            'Diesel': 1.05, 'Plug-in Hybrid': 1.20\n",
    "        }\n",
    "        fuel_factors = np.array([fuel_multipliers[fuel] for fuel in fuel_types])\n",
    "        \n",
    "        # Transmission adjustments\n",
    "        trans_multipliers = {\n",
    "            'Automatic': 1.0, 'Manual': 0.95, 'CVT': 0.98, 'Semi-Automatic': 1.1\n",
    "        }\n",
    "        trans_factors = np.array([trans_multipliers[trans] for trans in transmissions])\n",
    "        \n",
    "        # Drivetrain adjustments\n",
    "        drive_multipliers = {'FWD': 1.0, 'RWD': 1.05, 'AWD': 1.15, '4WD': 1.20}\n",
    "        drive_factors = np.array([drive_multipliers[drive] for drive in drivetrains])\n",
    "        \n",
    "        # Accident penalty\n",
    "        accident_factors = np.power(0.9, accident_counts)  # 10% reduction per accident\n",
    "        \n",
    "        # Previous owners penalty\n",
    "        owner_factors = np.power(0.97, prev_owners)  # 3% reduction per previous owner\n",
    "        \n",
    "        # Market demand adjustment\n",
    "        demand_multipliers = {'Low': 0.92, 'Medium': 1.0, 'High': 1.12}\n",
    "        demand_factors = np.array([demand_multipliers[demand] for demand in market_demand])\n",
    "        \n",
    "        # Calculate final prices\n",
    "        final_prices = (initial_prices * depreciation * mileage_factors * condition_factors * \n",
    "                       fuel_factors * trans_factors * drive_factors * accident_factors * \n",
    "                       owner_factors * demand_factors * seasonal_factor)\n",
    "        \n",
    "        # Add some realistic noise\n",
    "        noise = np.random.normal(1, 0.1, n_samples)\n",
    "        final_prices *= noise\n",
    "        \n",
    "        # Ensure reasonable price range\n",
    "        final_prices = np.clip(final_prices, 1000, 150000)\n",
    "        \n",
    "        return final_prices\n",
    "    \n",
    "    def advanced_feature_engineering(self, df):\n",
    "        \"\"\"\n",
    "        Create advanced features for better model performance\n",
    "        \"\"\"\n",
    "        df_engineered = df.copy()\n",
    "        \n",
    "        # Interaction features\n",
    "        df_engineered['age_mileage_interaction'] = df_engineered['vehicle_age'] * df_engineered['mileage']\n",
    "        df_engineered['luxury_age_interaction'] = df_engineered['luxury_brand'].astype(int) * df_engineered['vehicle_age']\n",
    "        df_engineered['condition_mileage_interaction'] = df_engineered['condition'].map({\n",
    "            'Poor': 1, 'Fair': 2, 'Good': 3, 'Very Good': 4, 'Excellent': 5\n",
    "        }) * df_engineered['mileage']\n",
    "        \n",
    "        # Polynomial features for key variables\n",
    "        df_engineered['mileage_squared'] = df_engineered['mileage'] ** 2\n",
    "        df_engineered['age_squared'] = df_engineered['vehicle_age'] ** 2\n",
    "        df_engineered['mileage_log'] = np.log1p(df_engineered['mileage'])\n",
    "        \n",
    "        # Binning continuous variables\n",
    "        df_engineered['mileage_bin'] = pd.cut(df_engineered['mileage'], \n",
    "                                            bins=[0, 25000, 50000, 100000, float('inf')], \n",
    "                                            labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "        \n",
    "        df_engineered['age_bin'] = pd.cut(df_engineered['vehicle_age'], \n",
    "                                        bins=[0, 3, 7, 15, float('inf')], \n",
    "                                        labels=['New', 'Recent', 'Mature', 'Old'])\n",
    "        \n",
    "        # Market segment classification\n",
    "        luxury_makes = ['BMW', 'Mercedes', 'Audi', 'Lexus', 'Acura', 'Infiniti', 'Cadillac', 'Lincoln', 'Jaguar', 'Land Rover']\n",
    "        economy_makes = ['Hyundai', 'Kia', 'Nissan', 'Chevrolet']\n",
    "        \n",
    "        df_engineered['market_segment'] = 'Mainstream'\n",
    "        df_engineered.loc[df_engineered['make'].isin(luxury_makes), 'market_segment'] = 'Luxury'\n",
    "        df_engineered.loc[df_engineered['make'].isin(economy_makes), 'market_segment'] = 'Economy'\n",
    "        \n",
    "        return df_engineered\n",
    "    \n",
    "    def comprehensive_model_training(self, df):\n",
    "        \"\"\"\n",
    "        Train multiple models with comprehensive evaluation\n",
    "        \"\"\"\n",
    "        print(\"Starting comprehensive model training...\")\n",
    "        \n",
    "        # Feature engineering\n",
    "        df_engineered = self.advanced_feature_engineering(df)\n",
    "        \n",
    "        # Prepare features\n",
    "        categorical_columns = ['make', 'body_type', 'fuel_type', 'transmission', 'drivetrain', \n",
    "                             'condition', 'engine_size', 'state', 'market_demand', 'mileage_bin', \n",
    "                             'age_bin', 'market_segment']\n",
    "        \n",
    "        numerical_columns = ['year', 'cylinders', 'mileage', 'accident_count', 'service_records', \n",
    "                           'previous_owners', 'seasonal_factor', 'vehicle_age', 'mileage_per_year',\n",
    "                           'depreciation_rate', 'age_mileage_interaction', 'luxury_age_interaction',\n",
    "                           'condition_mileage_interaction', 'mileage_squared', 'age_squared', 'mileage_log']\n",
    "        \n",
    "        boolean_columns = ['luxury_brand', 'electric_hybrid', 'high_performance']\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        df_processed = df_engineered.copy()\n",
    "        for col in categorical_columns:\n",
    "            if col not in self.label_encoders:\n",
    "                self.label_encoders[col] = LabelEncoder()\n",
    "            df_processed[col + '_encoded'] = self.label_encoders[col].fit_transform(df_processed[col])\n",
    "        \n",
    "        # Prepare final feature set\n",
    "        feature_columns = (numerical_columns + boolean_columns + \n",
    "                          [col + '_encoded' for col in categorical_columns])\n",
    "        \n",
    "        X = df_processed[feature_columns].fillna(0)\n",
    "        y = df_processed['price']\n",
    "        \n",
    "        # Remove outliers\n",
    "        z_scores = np.abs(zscore(y))\n",
    "        X = X[z_scores < 3]\n",
    "        y = y[z_scores < 3]\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "        \n",
    "        print(f\"Training set size: {X_train.shape[0]}\")\n",
    "        print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "        print(f\"Test set size: {X_test.shape[0]}\")\n",
    "        \n",
    "        # Scale features\n",
    "        self.scalers['standard'] = StandardScaler()\n",
    "        self.scalers['robust'] = RobustScaler()\n",
    "        self.scalers['minmax'] = MinMaxScaler()\n",
    "        \n",
    "        X_train_std = self.scalers['standard'].fit_transform(X_train)\n",
    "        X_val_std = self.scalers['standard'].transform(X_val)\n",
    "        X_test_std = self.scalers['standard'].transform(X_test)\n",
    "        \n",
    "        X_train_robust = self.scalers['robust'].fit_transform(X_train)\n",
    "        X_val_robust = self.scalers['robust'].transform(X_val)\n",
    "        X_test_robust = self.scalers['robust'].transform(X_test)\n",
    "        \n",
    "        # Train individual models\n",
    "        print(\"\\nTraining individual models...\")\n",
    "        individual_results = {}\n",
    "        \n",
    "        for name, model in self.algorithms.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            try:\n",
    "                # Choose appropriate scaling\n",
    "                if name in ['svr_rbf', 'svr_linear', 'knn', 'mlp']:\n",
    "                    X_tr, X_v, X_te = X_train_std, X_val_std, X_test_std\n",
    "                elif name in ['huber', 'ransac']:\n",
    "                    X_tr, X_v, X_te = X_train_robust, X_val_robust, X_test_robust\n",
    "                else:\n",
    "                    X_tr, X_v, X_te = X_train, X_val, X_test\n",
    "                \n",
    "                # Train model\n",
    "                model.fit(X_tr, y_train)\n",
    "                \n",
    "                # Predictions\n",
    "                train_pred = model.predict(X_tr)\n",
    "                val_pred = model.predict(X_v)\n",
    "                test_pred = model.predict(X_te)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = {\n",
    "                    'train_mae': mean_absolute_error(y_train, train_pred),\n",
    "                    'val_mae': mean_absolute_error(y_val, val_pred),\n",
    "                    'test_mae': mean_absolute_error(y_test, test_pred),\n",
    "                    'train_rmse': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "                    'val_rmse': np.sqrt(mean_squared_error(y_val, val_pred)),\n",
    "                    'test_rmse': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "                    'train_r2': r2_score(y_train, train_pred),\n",
    "                    'val_r2': r2_score(y_val, val_pred),\n",
    "                    'test_r2': r2_score(y_test, test_pred),\n",
    "                    'train_mape': mean_absolute_percentage_error(y_train, train_pred),\n",
    "                    'val_mape': mean_absolute_percentage_error(y_val, val_pred),\n",
    "                    'test_mape': mean_absolute_percentage_error(y_test, test_pred)\n",
    "                }\n",
    "                \n",
    "                individual_results[name] = metrics\n",
    "                self.models[name] = model\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error training {name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Train ensemble models\n",
    "        print(\"\\nTraining ensemble models...\")\n",
    "        ensemble_results = {}\n",
    "        \n",
    "        for ensemble_name, base_models in self.ensemble_configs.items():\n",
    "            print(f\"Training {ensemble_name}...\")\n",
    "            try:\n",
    "                # Voting ensemble (simple average)\n",
    "                if ensemble_name == 'voting_ensemble':\n",
    "                    ensemble_model = AdvancedEnsembleRegressor(\n",
    "                        base_models=base_models, \n",
    "                        use_stacking=False\n",
    "                    )\n",
    "                    ensemble_model.fit(X_train, y_train)\n",
    "                    \n",
    "                # Stacking ensemble\n",
    "                elif ensemble_name == 'stacking_ensemble':\n",
    "                    ensemble_model = AdvancedEnsembleRegressor(\n",
    "                        base_models=base_models,\n",
    "                        meta_model=Ridge(alpha=0.1),\n",
    "                        use_stacking=True\n",
    "                    )\n",
    "                    ensemble_model.fit(X_train, y_train)\n",
    "                    \n",
    "                # Meta ensemble with neural network meta-learner\n",
    "                elif ensemble_name == 'meta_ensemble':\n",
    "                    ensemble_model = AdvancedEnsembleRegressor(\n",
    "                        base_models=base_models,\n",
    "                        meta_model=MLPRegressor(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42),\n",
    "                        use_stacking=True\n",
    "                    )\n",
    "                    ensemble_model.fit(X_train, y_train)\n",
    "                \n",
    "                # Evaluate ensemble\n",
    "                train_pred = ensemble_model.predict(X_train)\n",
    "                val_pred = ensemble_model.predict(X_val)\n",
    "                test_pred = ensemble_model.predict(X_test)\n",
    "                \n",
    "                metrics = {\n",
    "                    'train_mae': mean_absolute_error(y_train, train_pred),\n",
    "                    'val_mae': mean_absolute_error(y_val, val_pred),\n",
    "                    'test_mae': mean_absolute_error(y_test, test_pred),\n",
    "                    'train_rmse': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "                    'val_rmse': np.sqrt(mean_squared_error(y_val, val_pred)),\n",
    "                    'test_rmse': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "                    'train_r2': r2_score(y_train, train_pred),\n",
    "                    'val_r2': r2_score(y_val, val_pred),\n",
    "                    'test_r2': r2_score(y_test, test_pred),\n",
    "                    'train_mape': mean_absolute_percentage_error(y_train, train_pred),\n",
    "                    'val_mape': mean_absolute_percentage_error(y_val, val_pred),\n",
    "                    'test_mape': mean_absolute_percentage_error(y_test, test_pred)\n",
    "                }\n",
    "                \n",
    "                ensemble_results[ensemble_name] = metrics\n",
    "                self.models[ensemble_name] = ensemble_model\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error training {ensemble_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Combine all results\n",
    "        all_results = {**individual_results, **ensemble_results}\n",
    "        self.model_performances = all_results\n",
    "        \n",
    "        # Find best model based on validation R²\n",
    "        best_model_name = max(all_results.keys(), key=lambda x: all_results[x]['val_r2'])\n",
    "        self.best_model = self.models[best_model_name]\n",
    "        \n",
    "        print(f\"\\nBest model: {best_model_name}\")\n",
    "        print(f\"Best validation R²: {all_results[best_model_name]['val_r2']:.4f}\")\n",
    "        \n",
    "        # Store feature columns for prediction\n",
    "        self.feature_columns = feature_columns\n",
    "        \n",
    "        return all_results, best_model_name\n",
    "    \n",
    "    def display_comprehensive_results(self):\n",
    "        \"\"\"\n",
    "        Display comprehensive model comparison results\n",
    "        \"\"\"\n",
    "        if not self.model_performances:\n",
    "            print(\"No model performance data available. Train models first.\")\n",
    "            return\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame(self.model_performances).T\n",
    "        \n",
    "        # Sort by validation R²\n",
    "        results_df = results_df.sort_values('val_r2', ascending=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"COMPREHENSIVE MODEL PERFORMANCE COMPARISON\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        print(f\"\\n{'Model':<20} {'Val R²':<8} {'Test R²':<8} {'Val MAE':<10} {'Test MAE':<10} {'Val RMSE':<10} {'Test RMSE':<10} {'Val MAPE':<8}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for model_name, metrics in results_df.iterrows():\n",
    "            print(f\"{model_name:<20} {metrics['val_r2']:<8.4f} {metrics['test_r2']:<8.4f} \"\n",
    "                  f\"{metrics['val_mae']:<10.0f} {metrics['test_mae']:<10.0f} \"\n",
    "                  f\"{metrics['val_rmse']:<10.0f} {metrics['test_rmse']:<10.0f} {metrics['val_mape']:<8.2f}%\")\n",
    "        \n",
    "        # Performance tiers\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODEL PERFORMANCE TIERS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        excellent_models = results_df[results_df['val_r2'] >= 0.95]\n",
    "        very_good_models = results_df[(results_df['val_r2'] >= 0.90) & (results_df['val_r2'] < 0.95)]\n",
    "        good_models = results_df[(results_df['val_r2'] >= 0.85) & (results_df['val_r2'] < 0.90)]\n",
    "        fair_models = results_df[results_df['val_r2'] < 0.85]\n",
    "        \n",
    "        if not excellent_models.empty:\n",
    "            print(f\"\\n🏆 EXCELLENT PERFORMANCE (R² ≥ 0.95): {len(excellent_models)} models\")\n",
    "            for model in excellent_models.index[:3]:  # Top 3\n",
    "                print(f\"   • {model}: R² = {excellent_models.loc[model, 'val_r2']:.4f}\")\n",
    "        \n",
    "        if not very_good_models.empty:\n",
    "            print(f\"\\n⭐ VERY GOOD PERFORMANCE (0.90 ≤ R² < 0.95): {len(very_good_models)} models\")\n",
    "            for model in very_good_models.index[:3]:\n",
    "                print(f\"   • {model}: R² = {very_good_models.loc[model, 'val_r2']:.4f}\")\n",
    "        \n",
    "        if not good_models.empty:\n",
    "            print(f\"\\n✓ GOOD PERFORMANCE (0.85 ≤ R² < 0.90): {len(good_models)} models\")\n",
    "            for model in good_models.index[:2]:\n",
    "                print(f\"   • {model}: R² = {good_models.loc[model, 'val_r2']:.4f}\")\n",
    "        \n",
    "        if not fair_models.empty:\n",
    "            print(f\"\\n△ FAIR PERFORMANCE (R² < 0.85): {len(fair_models)} models\")\n",
    "        \n",
    "        # Best model summary\n",
    "        best_model_name = results_df.index[0]\n",
    "        best_metrics = results_df.iloc[0]\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"BEST MODEL SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Model: {best_model_name}\")\n",
    "        print(f\"Validation R²: {best_metrics['val_r2']:.4f}\")\n",
    "        print(f\"Test R²: {best_metrics['test_r2']:.4f}\")\n",
    "        print(f\"Validation MAE: ${best_metrics['val_mae']:,.0f}\")\n",
    "        print(f\"Test MAE: ${best_metrics['test_mae']:,.0f}\")\n",
    "        print(f\"Validation MAPE: {best_metrics['val_mape']:.2f}%\")\n",
    "        print(f\"Test MAPE: {best_metrics['test_mape']:.2f}%\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def advanced_hyperparameter_tuning(self, df, top_models=3):\n",
    "        \"\"\"\n",
    "        Perform advanced hyperparameter tuning for top models\n",
    "        \"\"\"\n",
    "        print(f\"\\nPerforming hyperparameter tuning for top {top_models} models...\")\n",
    "        \n",
    "        if not self.model_performances:\n",
    "            print(\"Train models first before hyperparameter tuning.\")\n",
    "            return\n",
    "        \n",
    "        # Get top models\n",
    "        sorted_models = sorted(self.model_performances.items(), \n",
    "                             key=lambda x: x[1]['val_r2'], reverse=True)\n",
    "        top_model_names = [name for name, _ in sorted_models[:top_models]]\n",
    "        \n",
    "        # Prepare data\n",
    "        df_engineered = self.advanced_feature_engineering(df)\n",
    "        \n",
    "        categorical_columns = ['make', 'body_type', 'fuel_type', 'transmission', 'drivetrain', \n",
    "                             'condition', 'engine_size', 'state', 'market_demand', 'mileage_bin', \n",
    "                             'age_bin', 'market_segment']\n",
    "        \n",
    "        numerical_columns = ['year', 'cylinders', 'mileage', 'accident_count', 'service_records', \n",
    "                           'previous_owners', 'seasonal_factor', 'vehicle_age', 'mileage_per_year',\n",
    "                           'depreciation_rate', 'age_mileage_interaction', 'luxury_age_interaction',\n",
    "                           'condition_mileage_interaction', 'mileage_squared', 'age_squared', 'mileage_log']\n",
    "        \n",
    "        boolean_columns = ['luxury_brand', 'electric_hybrid', 'high_performance']\n",
    "        \n",
    "        # Encode and prepare features\n",
    "        df_processed = df_engineered.copy()\n",
    "        for col in categorical_columns:\n",
    "            df_processed[col + '_encoded'] = self.label_encoders[col].transform(df_processed[col])\n",
    "        \n",
    "        feature_columns = (numerical_columns + boolean_columns + \n",
    "                          [col + '_encoded' for col in categorical_columns])\n",
    "        \n",
    "        X = df_processed[feature_columns].fillna(0)\n",
    "        y = df_processed['price']\n",
    "        \n",
    "        # Remove outliers\n",
    "        z_scores = np.abs(zscore(y))\n",
    "        X = X[z_scores < 3]\n",
    "        y = y[z_scores < 3]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Hyperparameter grids\n",
    "        param_grids = {\n",
    "            'random_forest': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 15, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2', None]\n",
    "            },\n",
    "            'gradient_boosting': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [6, 8, 10],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'min_samples_split': [2, 5, 10]\n",
    "            },\n",
    "            'xgboost': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [6, 8, 10],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'lightgbm': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [6, 8, 10],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "            },\n",
    "            'extra_trees': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 15, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        tuned_models = {}\n",
    "        \n",
    "        for model_name in top_model_names:\n",
    "            if model_name in param_grids:\n",
    "                print(f\"Tuning {model_name}...\")\n",
    "                \n",
    "                # Get base model\n",
    "                base_model = self.algorithms[model_name]\n",
    "                \n",
    "                # Perform randomized search\n",
    "                random_search = RandomizedSearchCV(\n",
    "                    base_model,\n",
    "                    param_grids[model_name],\n",
    "                    n_iter=20,\n",
    "                    cv=5,\n",
    "                    scoring='r2',\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                \n",
    "                random_search.fit(X_train, y_train)\n",
    "                \n",
    "                # Best model\n",
    "                best_model = random_search.best_estimator_\n",
    "                \n",
    "                # Evaluate\n",
    "                train_pred = best_model.predict(X_train)\n",
    "                test_pred = best_model.predict(X_test)\n",
    "                \n",
    "                tuned_performance = {\n",
    "                    'best_params': random_search.best_params_,\n",
    "                    'train_r2': r2_score(y_train, train_pred),\n",
    "                    'test_r2': r2_score(y_test, test_pred),\n",
    "                    'train_mae': mean_absolute_error(y_train, train_pred),\n",
    "                    'test_mae': mean_absolute_error(y_test, test_pred),\n",
    "                    'improvement': r2_score(y_test, test_pred) - self.model_performances[model_name]['test_r2']\n",
    "                }\n",
    "                \n",
    "                tuned_models[model_name] = {\n",
    "                    'model': best_model,\n",
    "                    'performance': tuned_performance\n",
    "                }\n",
    "                \n",
    "                print(f\"  Original R²: {self.model_performances[model_name]['test_r2']:.4f}\")\n",
    "                print(f\"  Tuned R²: {tuned_performance['test_r2']:.4f}\")\n",
    "                print(f\"  Improvement: {tuned_performance['improvement']:.4f}\")\n",
    "        \n",
    "        return tuned_models\n",
    "    \n",
    "    def feature_importance_analysis(self):\n",
    "        \"\"\"\n",
    "        Analyze feature importance across different models\n",
    "        \"\"\"\n",
    "        if not self.models:\n",
    "            print(\"Train models first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        feature_importance_dict = {}\n",
    "        \n",
    "        # Tree-based models\n",
    "        tree_models = ['random_forest', 'gradient_boosting', 'xgboost', 'lightgbm', 'extra_trees']\n",
    "        \n",
    "        for model_name in tree_models:\n",
    "            if model_name in self.models:\n",
    "                model = self.models[model_name]\n",
    "                if hasattr(model, 'feature_importances_'):\n",
    "                    importances = model.feature_importances_\n",
    "                    feature_importance_dict[model_name] = dict(zip(self.feature_columns, importances))\n",
    "        \n",
    "        if feature_importance_dict:\n",
    "            # Calculate average importance\n",
    "            all_features = set()\n",
    "            for importances in feature_importance_dict.values():\n",
    "                all_features.update(importances.keys())\n",
    "            \n",
    "            avg_importance = {}\n",
    "            for feature in all_features:\n",
    "                scores = [feature_importance_dict[model].get(feature, 0) \n",
    "                         for model in feature_importance_dict.keys()]\n",
    "                avg_importance[feature] = np.mean(scores)\n",
    "            \n",
    "            # Sort by importance\n",
    "            sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(\"\\nTop 15 Most Important Features (Average across tree models):\")\n",
    "            print(\"-\" * 60)\n",
    "            for i, (feature, importance) in enumerate(sorted_features[:15], 1):\n",
    "                print(f\"{i:2d}. {feature:<35} {importance:.4f}\")\n",
    "            \n",
    "            self.feature_importance = avg_importance\n",
    "        \n",
    "        return feature_importance_dict\n",
    "    \n",
    "    def model_prediction_with_uncertainty(self, vehicle_features):\n",
    "        \"\"\"\n",
    "        Make predictions with uncertainty quantification using multiple models\n",
    "        \"\"\"\n",
    "        if not self.models:\n",
    "            print(\"Train models first.\")\n",
    "            return None\n",
    "        \n",
    "        # Prepare features\n",
    "        processed_features = self._prepare_prediction_features(vehicle_features)\n",
    "        \n",
    "        if processed_features is None:\n",
    "            return None\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        predictions = {}\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            try:\n",
    "                # Apply appropriate scaling\n",
    "                if model_name in ['svr_rbf', 'svr_linear', 'knn', 'mlp']:\n",
    "                    features_scaled = self.scalers['standard'].transform([processed_features])\n",
    "                elif model_name in ['huber', 'ransac']:\n",
    "                    features_scaled = self.scalers['robust'].transform([processed_features])\n",
    "                else:\n",
    "                    features_scaled = [processed_features]\n",
    "                \n",
    "                pred = model.predict(features_scaled)[0]\n",
    "                predictions[model_name] = max(pred, 1000)  # Minimum $1000\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting with {model_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not predictions:\n",
    "            return None\n",
    "        \n",
    "        # Calculate statistics\n",
    "        pred_values = list(predictions.values())\n",
    "        mean_prediction = np.mean(pred_values)\n",
    "        std_prediction = np.std(pred_values)\n",
    "        median_prediction = np.median(pred_values)\n",
    "        \n",
    "        # Get best model prediction\n",
    "        best_model_name = max(self.model_performances.keys(), \n",
    "                             key=lambda x: self.model_performances[x]['val_r2'])\n",
    "        best_prediction = predictions.get(best_model_name, mean_prediction)\n",
    "        \n",
    "        # Calculate confidence intervals\n",
    "        confidence_95 = {\n",
    "            'lower': mean_prediction - 1.96 * std_prediction,\n",
    "            'upper': mean_prediction + 1.96 * std_prediction\n",
    "        }\n",
    "        \n",
    "        confidence_68 = {\n",
    "            'lower': mean_prediction - std_prediction,\n",
    "            'upper': mean_prediction + std_prediction\n",
    "        }\n",
    "        \n",
    "        # Model agreement score\n",
    "        cv = std_prediction / mean_prediction if mean_prediction > 0 else 1\n",
    "        agreement_score = max(0, 100 * (1 - cv))  # Higher is better agreement\n",
    "        \n",
    "        return {\n",
    "            'predictions': predictions,\n",
    "            'best_model_prediction': best_prediction,\n",
    "            'ensemble_mean': mean_prediction,\n",
    "            'ensemble_median': median_prediction,\n",
    "            'ensemble_std': std_prediction,\n",
    "            'confidence_intervals': {\n",
    "                '68%': confidence_68,\n",
    "                '95%': confidence_95\n",
    "            },\n",
    "            'model_agreement_score': agreement_score,\n",
    "            'prediction_range': {\n",
    "                'min': min(pred_values),\n",
    "                'max': max(pred_values)\n",
    "            },\n",
    "            'best_model': best_model_name\n",
    "        }\n",
    "    \n",
    "    def _prepare_prediction_features(self, vehicle_features):\n",
    "        \"\"\"\n",
    "        Prepare features for prediction\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create base feature vector\n",
    "            processed_features = []\n",
    "            \n",
    "            # Numerical features\n",
    "            numerical_columns = ['year', 'cylinders', 'mileage', 'accident_count', 'service_records', \n",
    "                               'previous_owners', 'seasonal_factor', 'vehicle_age', 'mileage_per_year',\n",
    "                               'depreciation_rate', 'age_mileage_interaction', 'luxury_age_interaction',\n",
    "                               'condition_mileage_interaction', 'mileage_squared', 'age_squared', 'mileage_log']\n",
    "            \n",
    "            # Calculate derived features\n",
    "            vehicle_age = 2025 - vehicle_features.get('year', 2020)\n",
    "            mileage = vehicle_features.get('mileage', 50000)\n",
    "            \n",
    "            feature_dict = {\n",
    "                'year': vehicle_features.get('year', 2020),\n",
    "                'cylinders': vehicle_features.get('cylinders', 4),\n",
    "                'mileage': mileage,\n",
    "                'accident_count': vehicle_features.get('accident_count', 0),\n",
    "                'service_records': vehicle_features.get('service_records', vehicle_age),\n",
    "                'previous_owners': vehicle_features.get('previous_owners', 1),\n",
    "                'seasonal_factor': vehicle_features.get('seasonal_factor', 1.0),\n",
    "                'vehicle_age': vehicle_age,\n",
    "                'mileage_per_year': mileage / max(vehicle_age, 1),\n",
    "                'depreciation_rate': 1 - (0.15 * vehicle_age),\n",
    "                'age_mileage_interaction': vehicle_age * mileage,\n",
    "                'luxury_age_interaction': (1 if vehicle_features.get('make', '') in \n",
    "                                         ['BMW', 'Mercedes', 'Audi', 'Lexus'] else 0) * vehicle_age,\n",
    "                'condition_mileage_interaction': {'Poor': 1, 'Fair': 2, 'Good': 3, 'Very Good': 4, \n",
    "                                                'Excellent': 5}.get(vehicle_features.get('condition', 'Good'), 3) * mileage,\n",
    "                'mileage_squared': mileage ** 2,\n",
    "                'age_squared': vehicle_age ** 2,\n",
    "                'mileage_log': np.log1p(mileage)\n",
    "            }\n",
    "            \n",
    "            # Add numerical features\n",
    "            for col in numerical_columns:\n",
    "                processed_features.append(feature_dict.get(col, 0))\n",
    "            \n",
    "            # Boolean features\n",
    "            luxury_makes = ['BMW', 'Mercedes', 'Audi', 'Lexus', 'Acura', 'Infiniti', 'Cadillac', 'Lincoln', 'Jaguar', 'Land Rover']\n",
    "            electric_hybrid = ['Electric', 'Hybrid', 'Plug-in Hybrid']\n",
    "            \n",
    "            processed_features.extend([\n",
    "                1 if vehicle_features.get('make', '') in luxury_makes else 0,\n",
    "                1 if vehicle_features.get('fuel_type', '') in electric_hybrid else 0,\n",
    "                1 if vehicle_features.get('cylinders', 4) >= 8 else 0\n",
    "            ])\n",
    "            \n",
    "            # Categorical features\n",
    "            categorical_columns = ['make', 'body_type', 'fuel_type', 'transmission', 'drivetrain', \n",
    "                                 'condition', 'engine_size', 'state', 'market_demand', 'mileage_bin', \n",
    "                                 'age_bin', 'market_segment']\n",
    "            \n",
    "            for col in categorical_columns:\n",
    "                value = vehicle_features.get(col, '')\n",
    "                if col in self.label_encoders:\n",
    "                    try:\n",
    "                        encoded_value = self.label_encoders[col].transform([value])[0]\n",
    "                    except (ValueError, KeyError):\n",
    "                        encoded_value = 0  # Unknown category\n",
    "                else:\n",
    "                    encoded_value = 0\n",
    "                processed_features.append(encoded_value)\n",
    "            \n",
    "            return processed_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing features: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def comprehensive_vehicle_analysis(self, vehicle_data):\n",
    "        \"\"\"\n",
    "        Perform comprehensive vehicle analysis with all verification and prediction\n",
    "        \"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"COMPREHENSIVE VEHICLE ANALYSIS REPORT\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Vehicle details\n",
    "        print(f\"\\nVehicle: {vehicle_data.get('year', 'Unknown')} {vehicle_data.get('make', 'Unknown')} {vehicle_data.get('model', 'Unknown')}\")\n",
    "        print(f\"VIN: {vehicle_data.get('vin', 'Not provided')}\")\n",
    "        print(f\"Mileage: {vehicle_data.get('mileage', 'Unknown'):,} miles\")\n",
    "        print(f\"Condition: {vehicle_data.get('condition', 'Unknown')}\")\n",
    "        \n",
    "        # Price prediction with uncertainty\n",
    "        prediction_result = self.model_prediction_with_uncertainty(vehicle_data)\n",
    "        \n",
    "        if prediction_result:\n",
    "            print(f\"\\n\" + \"=\"*50)\n",
    "            print(\"PRICE PREDICTION ANALYSIS\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            print(f\"Best Model ({prediction_result['best_model']}):\")\n",
    "            print(f\"  Predicted Price: ${prediction_result['best_model_prediction']:,.2f}\")\n",
    "            \n",
    "            print(f\"\\nEnsemble Analysis:\")\n",
    "            print(f\"  Mean Prediction: ${prediction_result['ensemble_mean']:,.2f}\")\n",
    "            print(f\"  Median Prediction: ${prediction_result['ensemble_median']:,.2f}\")\n",
    "            print(f\"  Prediction Range: ${prediction_result['prediction_range']['min']:,.2f} - ${prediction_result['prediction_range']['max']:,.2f}\")\n",
    "            \n",
    "            print(f\"\\nConfidence Intervals:\")\n",
    "            print(f\"  68% CI: ${prediction_result['confidence_intervals']['68%']['lower']:,.2f} - ${prediction_result['confidence_intervals']['68%']['upper']:,.2f}\")\n",
    "            print(f\"  95% CI: ${prediction_result['confidence_intervals']['95%']['lower']:,.2f} - ${prediction_result['confidence_intervals']['95%']['upper']:,.2f}\")\n",
    "            \n",
    "            print(f\"\\nModel Agreement Score: {prediction_result['model_agreement_score']:.1f}%\")\n",
    "            \n",
    "            # Individual model predictions\n",
    "            print(f\"\\nIndividual Model Predictions:\")\n",
    "            for model, pred in sorted(prediction_result['predictions'].items(), \n",
    "                                   key=lambda x: x[1], reverse=True):\n",
    "                print(f\"  {model:<20}: ${pred:,.2f}\")\n",
    "        \n",
    "        return prediction_result\n",
    "\n",
    "# Example usage and comprehensive testing\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing Advanced Vehicle System...\")\n",
    "    vehicle_system = AdvancedVehicleSystem()\n",
    "    \n",
    "    # Generate comprehensive dataset\n",
    "    print(\"Generating advanced dataset...\")\n",
    "    df = vehicle_system.generate_advanced_dataset(n_samples=30000)\n",
    "    print(f\"Generated dataset with {len(df)} samples and {len(df.columns)} features\")\n",
    "    \n",
    "    # Train all models\n",
    "    print(\"\\nStarting comprehensive model training...\")\n",
    "    results, best_model = vehicle_system.comprehensive_model_training(df)\n",
    "    \n",
    "    # Display results\n",
    "    results_summary = vehicle_system.display_comprehensive_results()\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    feature_importance = vehicle_system.feature_importance_analysis()\n",
    "    \n",
    "    # Hyperparameter tuning for top models\n",
    "    tuned_models = vehicle_system.advanced_hyperparameter_tuning(df, top_models=3)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    for model_name, tuning_result in tuned_models.items():\n",
    "        perf = tuning_result['performance']\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Best Parameters: {perf['best_params']}\")\n",
    "        print(f\"  Test R² Improvement: {perf['improvement']:+.4f}\")\n",
    "        print(f\"  Final Test R²: {perf['test_r2']:.4f}\")\n",
    "    \n",
    "    # Example comprehensive analysis\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE VEHICLE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_vehicle = {\n",
    "        'vin': '1HGCM82633A123456',\n",
    "        'year': 2020,\n",
    "        'make': 'Toyota',\n",
    "        'model': 'Camry',\n",
    "        'body_type': 'Sedan',\n",
    "        'fuel_type': 'Gasoline',\n",
    "        'transmission': 'Automatic',\n",
    "        'drivetrain': 'FWD',\n",
    "        'condition': 'Good',\n",
    "        'engine_size': '2.5L',\n",
    "        'cylinders': 4,\n",
    "        'mileage': 45000,\n",
    "        'state': 'CA',\n",
    "        'accident_count': 0,\n",
    "        'service_records': 8,\n",
    "        'previous_owners': 1,\n",
    "        'market_demand': 'Medium',\n",
    "        'seasonal_factor': 1.02\n",
    "    }\n",
    "    \n",
    "    analysis_result = vehicle_system.comprehensive_vehicle_analysis(sample_vehicle)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SYSTEM PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total Models Trained: {len(vehicle_system.models)}\")\n",
    "    print(f\"Best Model: {best_model}\")\n",
    "    print(f\"Best Validation R²: {max(r['val_r2'] for r in vehicle_system.model_performances.values()):.4f}\")\n",
    "    print(f\"Dataset Size: {len(df):,} samples\")\n",
    "    print(f\"Feature Count: {len(vehicle_system.feature_columns)} features\")\n",
    "    \n",
    "    # Model complexity comparison\n",
    "    print(f\"\\nModel Complexity Analysis:\")\n",
    "    ensemble_models = [name for name in vehicle_system.models.keys() if 'ensemble' in name]\n",
    "    individual_models = [name for name in vehicle_system.models.keys() if 'ensemble' not in name]\n",
    "    \n",
    "    if ensemble_models:\n",
    "        ensemble_r2 = [vehicle_system.model_performances[name]['val_r2'] for name in ensemble_models]\n",
    "        print(f\"  Average Ensemble R²: {np.mean(ensemble_r2):.4f}\")\n",
    "    \n",
    "    if individual_models:\n",
    "        individual_r2 = [vehicle_system.model_performances[name]['val_r2'] for name in individual_models]\n",
    "        print(f\"  Average Individual R²: {np.mean(individual_r2):.4f}\")\n",
    "    \n",
    "    print(f\"\\nSystem ready for production use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20b7382-e393-484c-8e8e-6ecb6e36bcae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
